= Learning Rama: Day 3 - foreign keys and data integrity
:docs: https://redplanetlabs.com/docs/~/

The adventure continues! In the xref:./day2.adoc[previous installment], I have created a simple C\(R)UD for "components". Today, we will spice it up and add a foreign key and data integrity maintenance. Namely, a component can have a parent, which can also have a parent, etc. I will implement two features:

. Ensuring that `parent` is a valid component ID when creating or updating the value
. A query to retrieve the full hierarchy of a component's descendants (so that it can be checked prior to a delete operation)
 * I may want to have a computed property `has-children?` to know whether to check for descendants
. When deleting a parent component, deleting the whole hierarchy of its descendants
 * Likely, I need to check for children upon delete (could have got some while the deletion was issued)
 * Should I check that it hasn't got any new descendants since a user approved the deletion? Or perhaps we should delete all approved and new anyway? Or support a `force` option to delete also potential new descendants without approval?

== Core design

I already have `id -> component` PState and likely would want a `comp-id -> #{children IDs}` one. Rama by default pre-computes the length of sub-indexed subsequences, which would be helpful for `has-children?`, if I want to subindex it (i.e. if it is expected to tend to have over 100 elements - which I don't think is the case, though it could happen occasionally.) I could also maintain another PState, `comp-id -> count-of-children`... . Though not sure count of children is actually ever interesting. Count of all descendants could be, but that would also be more expensive to maintain. A simple has-children? might be enough.

IMO subindexing doesn't make sense here - I expect far only units or 10s of children on average, and mostly I will want to read all of them anyway. It could be interesting to store the parent-child relationship as a tree, though I don't think that is possible (since I'd essentially need a recursive schema). I guess I could cheat and use `(map-schema String clojure.lang.PersistentArrayMap)` or something, but updating it might be expensive, and finding a particular place in the tree (i.e. the subtree for a particular descendant) would be inefficient, requiring the whole root component's tree to be read.

=== Questions

Calculating descendant subtree - is Rama smart enough to do this efficiently, i.e. in each iteration compute children at level N (on each node in parallel), then combine and redistribute the output so that all grand-children that hash to the same partition are loaded at the same time (instead of each being processed separately)?

== Learnings

=== Questions explored

. How to create a conditional sub-tree of computation? (I.e. if the input has `:parent`, go and check it)
. How to return early from the topology when the input is invalid? (E.g. the provided parent id does not exist.)
. How to check for existence? (I.e. "Does the given parent to be set actually exist?") A: (https://clojurians.slack.com/archives/C05N2M7R6DB/p1709681282921649?thread_ts=1709673534.904289&cid=C05N2M7R6DB[thx, Nathan!]): use `(view ...)` as the whole path: `(local-select> (view contains? :x) ++ps :> *present?)`
 * BEWARE: This doesn't work with `select>` b/c it wouldn't know what to partition to (it needs a `(keypath ...)` for that, I assume). So I need to manually use `|hash + local-select>`.
 * NOTE: `local-select>` w/ `keypath` returns `nil` upon no match, so I can do `(<<if *the-result ....)` later on.
. How to select on a potentially different partition and then come back? A: `select>` then `(|hash ...)`.
. How do I add an element to a (possibly nonexistent) set? A: `(local-transform> [(keypath *parent) NONE-ELEM (termval *comp-id)] ++children)`
. _Transactional update across multiple partitions_ - see below
. Reusability: how to make a `valid-parent?` fragment that I could use both in the create and update flows?
. _Query topologies_ - creating them and using them in ETLs (to perform data validity checks) - see below

=== Tidbits

* `ifexpr` can be used with `:>` and a missing then-branch will produce `nil`, just like in clj: `(ifexpr false "..." :> *error) ; *error is nil`
* link:{docs}+clj-dataflow-lang.html#_loops+[From the docs]: "`[..] the body [of a loop] can call `continue>` multiple times in one iteration. An example use case where this comes up is a query topology doing a parallel graph search, where each iteration of the loop continues along all outgoing edges.`" This sounds very powerful, compared to ordinary Clojure...

=== Transactional update across multiple partitions

==== Background

Topologies are triggered by depot appends. Rama guarantees that appends to a particular partition of a particular depot will be processed in the order of arrival. Appends to different partitions or to different depots that map to the same partition (node) do not have any such guarantee.

A single depot append may link:{docs}+stream.html#_operation+[result in a tree of _events_] and is only processed when they all finish successfully. link:{docs}+intermediate-dataflow.html#_yieldifovertime+[What is an event]? It is all the code in between two _async boundaries_: "`An async boundary is where code moves to another task, or where a remote operation is performed. Async boundaries in Rama [.line-through]#can be# _is one of_ partitioners, localSelect calls on mirror PStates, or https://redplanetlabs.com/docs/~/integrating.html[calls to external queues or databases].`"

All writes within a single event are atomic - i.e. all writes to any number of PStates (remember, on the same partition) are made visible at the same time. At least for stream topologies - in microbatching, every PState update across all partitions is atomic.

This means that if a stream topology writes to multiple partitions, these writes become visible at different times, and not all at once. So what do we do if we want transactional semantics across multiple partitions, and the tradeoffs of microbatching are not acceptable, because we need a few-ms response time?

==== The problem

When creating a Component entity, which has another Component as its `:parent`, I want transactional semantics to ensure data integrity. Namely:

. I want to check that the parent component exists
. I want to add the new component to the parent's children set
. I want obviously to persist the child

Now, everything related to the parent happens possibly on a different partition than the child. Many things can fail here:

* The parent could have been deleted after I checked it but before (or even after) I persist the child
* The creation of the child could fail, if it fails to satisfy business rules - and the parent might thus and up pointing to a non-existent child

I do not want either a parent or a child pointing to something that does not exist.

==== Solution 1: Microbatching

As Nathan pointed out, every PState update across all partitions is transactional in microbatching, which makes this rather simple.

===== On microbatch topologies

> A microbatch iteration can take anywhere from a couple hundred milliseconds to many seconds, depending on the complexity of processing and the amount of incoming data.
>
> -- link:{docs}+tutorial5.html#_microbatch_topologies+[Tutorial: Microbatch topologies]

(I assume this link:{docs}+microbatch.html#_tuning_options+[can be tuned] with options such as `depot.microbatch.max.records`.)

> They [microbatch topologies] have significant additional capabilities for expressing computations [such as link:{docs}+intermediate-dataflow.html#_batch_blocks+[batch blocks]], different performance characteristics, and simple exactly-once fault-tolerance semantics. [I.e. an all-or-nothing atomicity.]

As link:{docs}+microbatch.html#_operation_and_fault_tolerance+[microbatch operation explains]:

> This means all changes to all PStates on a task in a microbatch become visible at the exact same time (though changes on different tasks [~ machines] may become visible at slightly different times).

NOTE: If you do depot appends as part of your microbatch topology [..], those currently do not have exactly-once semantics in the face of failures and retries. However, this is on our roadmap. [As of March 2024.]

> Unless you require millisecond-level update latency for your PStates, you should generally prefer microbatch topologies. They have higher throughput and simpler fault-tolerance semantics than stream topologies. 

==== Solution 2: Streaming

Let's (artificially) assume that component creation needs very short response time and thus we need to use a streaming topology. Here is a possible solution (where `[xxx]` denotes a partition):

. [parent] Check that the parent exists, and add the child's id to the `parent->future-children` PState
.. When you ask a parent about its children, this PState is ignored but if the parent is being deleted, it also schedules the deletion of its future children, by appending them to the appropriate depot
... Here it could be beneficial to have creates and deletes in the same depot, so that we do not risk the delete being processed before the create finishes (and thus failing to delete the not-yet-existing child) [TBD: confirm]
. [child] Check and persist the child
. [parent] Based on the situation:
.. If the child creation succeeded and the parent still exists then move the child to the `parent->children` PState
... Here we have a tiny moment where a child exists but its parent doesn't show it yet, but that's OK, the eventual consistency here is not a problem for me
... If the parent has been deleted in the meantime, then a removal of the child is already scheduled. The child may appear to some clients for a brief moment.
.. If the child creation failed, remove the child from the `parent->future-children` PState

.Stream topologies must be retriable
****
Any part of a distributed computation such as a stream topology may fail. Rama link:{docs}+stream.html#_fault_tolerance_and_retry_modes+[solves that by retrying] such a topology from scratch. Therefore the topology must be idempotent, i.e. it must be safe to run it multiple times, and it must be able to pick up from where it failed. This implies that we must modify data in the right order. In my case, when deleting a parent, I may only delete the `$$parent->children` entry after the successful deletion of all the children. (And make sure that an attempt to delete a deleted child does nothing.)
****

==== Open questions

How to test different "interleavings" of events, to make sure I never get into a state that would violate data integrity? RPL has a fascinating blog https://blog.redplanetlabs.com/2023/10/24/how-rama-is-tested-a-primer-on-testing-distributed-systems/[post about testing concurrent systems] but it is not clear to me whether/how I could leverage that for my tests.

=== Code reuse: deframafn vs. segmacro

I wanted to factor out `parent-error` to check whether component's `:parent` exists, if provided. But I learned I cannot use deframafn / deframaop for that because neither may contain partitioners. The solution is to use the inline `<<ramafn`, which did not fit here, or a link:{docs}+clj-dataflow-lang.html#_segmacros+[segmacro].

Pitfalls I encountered:

* Segmacro run at compile time and must not contain any nested function calls (obviously ðŸ˜…), unless you want them evaluated at compile time. You can turn `(f ...)` into `(seg# f ...)` to postpone its evaluation until runtime.
* Keywords cannot be used as fns, so `(seg# :kwd thing)` https://clojurians.slack.com/archives/C05N2M7R6DB/p1710201537795019[is invalid], you need to use `get` instead
* Partitioners cannot be used in pure dataflow `(?<- ...)`, need to actually run the module for that

Many thanks to Nathan for all the help!

=== Query topologies

Goal: When setting a parent, check that it won't create a loop in the hierarchy.

Extended goal: Give a child id, return the chain of its ancestors, starting with its parent. Given the capabilities of Rama, it should be simple, if not trivial, to compute ancestor chains for any number of children in parallel. I've actually decided to start with this, because it is interesting and creating a simpler version of the original need should then be simple.

Off to re-read link:{docs}+clj-defining-modules.html#_declaring_query_topologies+[Query topologies]! (Well, right after I refactor all links in this document with help of Asciidoc attributes, and find out how to make Vivaldi force RPL docs into dark mode ðŸ˜…)

Idea: Use a temporary PState or something similar with input-child-id -> ancestors. After each step, come back to the input id's partition and append the new ancestor. Return the PState value as the output.

Learnings:

* Define, remotely invoke a query topology
* Do not confuse `local-select>` and `select>`, it is the latter that does change partition ðŸ˜…
* How to use `loop<-`
* I have quite struggled with writing the code to collect all the ancestors. I've ended up with a DIY solution where the loop passes an accumulator around and only emits once it is done. I would have preferred to emit every parent found from the loop, and use something like `aggs/+vec-agg` to collect them, but couldn't figure out how to. It also took a while to figure out that I first need to go back to the origin partition and then use `aggs/+map-agg` to combine all the child + its ancestors pair into a single output map. (I originally had `(identity {*child (not-empty *ancestors)} :> *child->ancestors)` prior to that, but that emitted a single-element map for each child, while queries must produce exactly one output.)

Other ideas considered:

* Leverage link:{docs}+query.html#_temporary_in_memory_state_for_query_topologies+[Temporary in-memory state for query topologies] to collect the output
* Use a helper, recursive query topology instead of the loop

=== Tip: One entity = one depot

I've started by having 3 separate depots for Component Create, Update, and Delete operations, b/c that is what I saw in some of the examples. However, it seems cleaner to me to have just a single one, as it will then provide a single source of truth of the entity. https://clojurians.slack.com/archives/C05N2M7R6DB/p1709945183106729[Nathan approves]:

> yes, in general it's better to have the same entity managed through the same depot, particularly updates and deletes.
> Putting creates and updates on different depots usually won't have ordering problems because in most apps you can't update or delete something until it's been created

=== foreign-append! returns after topologies finish, even if they move to other partitions

The `foreign-append!` docstring reads "`waits for data to be appended and replicated to depot partition and for all colocated stream topologies to finish processing it`", which I misunderstood as "the processing on the local partition". But as https://clojurians.slack.com/archives/C05N2M7R6DB/p1709591831009549?thread_ts=1709591725.773629&cid=C05N2M7R6DB[Nathan kindly explained], the append call only returns after the topology has completely finished, even if it is using partitioners or doing mirror calls.

=== From the docs

==== Dataflow lang in clj

[quote]
____
Dataflow code consists of a sequence of "segments", analogous to a "form" in Clojure (since Rama dataflow is still Clojure, segments are also technically forms). A segment consists of an _operation_, _input fields_, and any number of "_output declarations_". An "output declaration" begins with an "output stream" followed by an optional "anchor" and any number of "variables" to bind for emits to that stream. Here are some examples of segments:

[source,clojure]
----
(+ 1 2 3 :> *sum) ; output 6 into the default stream as *sum

;; output streams :>, :a>, and :b>
(bar :a> <aaa> *v1 *v2 ; emit 2 fields to the stream a, anchor aaa
:b> <anchor-b> *v2 ; emit a filed, anchor anchor-b
:> *a) ; emit a field to the default stream

(println "Hello") ; 0 output declarations
----
____

> A "variable" is a symbol beginning with `*`, `%`, or `pass:[$$]`. * signifies a value, % signifies an anonymous operation, and pass:[$$] signifies a PState.